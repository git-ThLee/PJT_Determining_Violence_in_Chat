{"cells":[{"cell_type":"markdown","id":"af28a4f7","metadata":{"id":"af28a4f7"},"source":["참고\n","\n","https://luv-bansal.medium.com/fine-tuning-bert-for-text-classification-in-pytorch-503d97342db2,\n","\n","https://mccormickml.com/2019/07/22/BERT-fine-tuning/"]},{"cell_type":"code","execution_count":15,"id":"V4HbnGPWgKLs","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3981,"status":"ok","timestamp":1659513991051,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"V4HbnGPWgKLs","outputId":"6aa8296a-4957-4f3a-b1d9-1553bce9439c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":16,"id":"36c8df6c","metadata":{"ExecuteTime":{"end_time":"2022-07-29T01:01:45.700861Z","start_time":"2022-07-29T01:00:39.056850Z"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1659513991052,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"36c8df6c","scrolled":true},"outputs":[],"source":["import transformers\n","from transformers import BertTokenizer, BertModel\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data import TensorDataset, random_split\n","import torch\n","from torch import nn\n","from torch.optim import Adam\n","from tqdm import tqdm\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":17,"id":"fSTnTW7ld0gK","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1659513991766,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"fSTnTW7ld0gK"},"outputs":[],"source":["import torch, gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","id":"uuuih18kbr7X","metadata":{"id":"uuuih18kbr7X"},"source":["# GPU 할당"]},{"cell_type":"code","execution_count":18,"id":"f7c4fa5a","metadata":{"ExecuteTime":{"end_time":"2022-07-29T01:01:45.956880Z","start_time":"2022-07-29T01:01:45.702815Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1659513991766,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"f7c4fa5a","outputId":"88c0a994-8e95-4adb-9ebf-1103a2fdd82c"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"]}],"source":["# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":19,"id":"fwVSK412gRR-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2459,"status":"ok","timestamp":1659513994218,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"fwVSK412gRR-","outputId":"62eff18e-cfe5-47d2-f0c2-4e61606b9d90"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","id":"IgOcSDhCb3Rg","metadata":{"id":"IgOcSDhCb3Rg"},"source":["# 데이터 준비"]},{"cell_type":"code","execution_count":20,"id":"1da63fe8","metadata":{"ExecuteTime":{"end_time":"2022-07-29T01:03:22.365746Z","start_time":"2022-07-29T01:03:22.361838Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1659513994218,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"1da63fe8"},"outputs":[],"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":21,"id":"75b9e4d7","metadata":{"ExecuteTime":{"end_time":"2022-07-29T01:03:22.603850Z","start_time":"2022-07-29T01:03:22.598964Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1659513994218,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"75b9e4d7"},"outputs":[],"source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":22,"id":"e1dbe754","metadata":{"ExecuteTime":{"end_time":"2022-07-29T01:01:51.287038Z","start_time":"2022-07-29T01:01:51.280197Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1659513994219,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"e1dbe754"},"outputs":[],"source":["def make_input(df):\n","    sentences = df['문장'].tolist()\n","    input_ids =[]\n","    attention_masks =[]\n","    token_type_ids =[]\n","    for line in tqdm(sentences):\n","    #     line = ' '.join(mecab.morphs(line)) # mecab 적용, encode하면 tokenizer.tokenize 해준 것과 같은 결과 나옴\n","        encoded_dict = tokenizer.encode_plus(line, \\\n","                                             add_special_tokens = True,\\\n","                                             pad_to_max_length=True,\\\n","                                             max_length=MAX_LENGTH, \n","                                            return_attention_mask=True,\n","                                            truncation = True)\n","\n","        input_id=encoded_dict['input_ids']\n","        attention_mask=encoded_dict['attention_mask']\n","        token_type_id = encoded_dict['token_type_ids']\n","\n","        input_ids.append(input_id)\n","        attention_masks.append(attention_mask)\n","        token_type_ids.append(token_type_id)\n","    \n","    input_ids = torch.tensor(input_ids, dtype=torch.long).to(device)\n","    attention_masks = torch.tensor(attention_masks, dtype=torch.long).to(device)\n","    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).to(device)\n","    inputs = (input_ids, attention_masks, token_type_ids)\n","\n","    # print(\"Original Text : \", sentences[0])\n","    # print(\"Tokenizer Text : \", tokenizer.tokenize(sentences[0]))\n","    # print(\"Encode Text : \", (tokenizer.encode(sentences[0], add_special_tokens = True, max_length=MAX_LENGTH)))\n","    \n","    return TensorDataset(input_ids, attention_masks, token_type_ids, torch.tensor(df['violence'].tolist(), dtype=torch.long).to(device))"]},{"cell_type":"code","execution_count":23,"id":"4b39fc8f","metadata":{"ExecuteTime":{"start_time":"2022-07-29T01:03:23.013Z"},"executionInfo":{"elapsed":715,"status":"ok","timestamp":1659513994929,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"4b39fc8f"},"outputs":[],"source":["def training(train_dataloader, validation_dataloader):\n","  # This training code is based on the `run_glue.py` script here:\n","  # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","  # Set the seed value all over the place to make this reproducible.\n","  seed_val = 42\n","\n","  random.seed(seed_val)\n","  np.random.seed(seed_val)\n","  torch.manual_seed(seed_val)\n","  torch.cuda.manual_seed_all(seed_val)\n","\n","  # We'll store a number of quantities such as training and validation loss, \n","  # validation accuracy, and timings.\n","  training_stats = []\n","\n","  # Measure the total training time for the whole run.\n","  total_t0 = time.time()\n","\n","  # TensorDataset(input_ids, attention_masks, token_type_ids, torch.tensor(df['violence'].tolist(), dtype=torch.long).to(device))\n","\n","  # For each epoch...\n","  for epoch_i in range(0, epochs):\n","      \n","      # ========================================\n","      #               Training\n","      # ========================================\n","      \n","      # Perform one full pass over the training set.\n","\n","      # print(\"\")\n","      # print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","      # print('Training...')\n","\n","      # Measure how long the training epoch takes.\n","      t0 = time.time()\n","\n","      # Reset the total loss for this epoch.\n","      total_train_loss = 0\n","\n","      # Put the model into training mode. Don't be mislead--the call to \n","      # `train` just changes the *mode*, it doesn't *perform* the training.\n","      # `dropout` and `batchnorm` layers behave differently during training\n","      # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","      model.train()\n","\n","      # For each batch of training data...\n","      for step, batch in enumerate(train_dataloader):\n","\n","          # Progress update every 40 batches.\n","          if step % 50 == 0 and not step == 0:\n","              # Calculate elapsed time in minutes.\n","              elapsed = format_time(time.time() - t0)\n","              \n","              # Report progress.\n","              # print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","          # Unpack this training batch from our dataloader. \n","          #\n","          # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","          # `to` method.\n","          #\n","          # `batch` contains three pytorch tensors:\n","          #   [0]: input ids \n","          #   [1]: attention masks\n","          #   [2]: token_type_ids\n","          #   [3]: labels \n","          b_input_ids = batch[0].to(device)\n","          b_input_mask = batch[1].to(device)\n","          b_token_type_ids = batch[2].to(device)\n","          b_labels = batch[3].to(device)\n","\n","          # Always clear any previously calculated gradients before performing a\n","          # backward pass. PyTorch doesn't do this automatically because \n","          # accumulating the gradients is \"convenient while training RNNs\". \n","          # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","          model.zero_grad()        \n","\n","          # Perform a forward pass (evaluate the model on this training batch).\n","          # The documentation for this `model` function is here: \n","          # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","          # It returns different numbers of parameters depending on what arguments\n","          # arge given and what flags are set. For our useage here, it returns\n","          # the loss (because we provided labels) and the \"logits\"--the model\n","          # outputs prior to activation.\n","          loss, logits = model(b_input_ids, \n","                              token_type_ids=b_token_type_ids, \n","  #                              token_type_ids=None, \n","                              attention_mask=b_input_mask, \n","                              labels=b_labels)\n","\n","          # Accumulate the training loss over all of the batches so that we can\n","          # calculate the average loss at the end. `loss` is a Tensor containing a\n","          # single value; the `.item()` function just returns the Python value \n","          # from the tensor.\n","          total_train_loss += loss.item()\n","\n","          # Perform a backward pass to calculate the gradients.\n","          loss.backward()\n","\n","          # Clip the norm of the gradients to 1.0.\n","          # This is to help prevent the \"exploding gradients\" problem.\n","          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","          # Update parameters and take a step using the computed gradient.\n","          # The optimizer dictates the \"update rule\"--how the parameters are\n","          # modified based on their gradients, the learning rate, etc.\n","          optimizer.step()\n","\n","          # Update the learning rate.\n","          scheduler.step()\n","\n","      # Calculate the average loss over all of the batches.\n","      avg_train_loss = total_train_loss / len(train_dataloader)            \n","      \n","      # Measure how long this epoch took.\n","      training_time = format_time(time.time() - t0)\n","\n","      print(\"\")\n","      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","      print(\"  Training epcoh took: {:}\".format(training_time))\n","          \n","      # ========================================\n","      #               Validation\n","      # ========================================\n","      # After the completion of each training epoch, measure our performance on\n","      # our validation set.\n","\n","      # print(\"\")\n","      # print(\"Running Validation...\")\n","\n","      t0 = time.time()\n","\n","      # Put the model in evaluation mode--the dropout layers behave differently\n","      # during evaluation.\n","      model.eval()\n","\n","      # Tracking variables \n","      total_eval_accuracy = 0\n","      total_eval_loss = 0\n","      nb_eval_steps = 0\n","\n","      # Evaluate data for one epoch\n","      for batch in validation_dataloader:\n","          \n","          # Unpack this training batch from our dataloader. \n","          #\n","          # As we unpack the batch, we'll also copy each tensor to the GPU using \n","          # the `to` method.\n","          #\n","          # `batch` contains three pytorch tensors:\n","          #   [0]: input ids \n","          #   [1]: attention masks\n","          #   [2]: token_type_ids\n","          #   [3]: labels \n","          b_input_ids = batch[0].to(device)\n","          b_input_mask = batch[1].to(device)\n","          b_token_type_ids = batch[2].to(device)\n","          b_labels = batch[3].to(device)\n","          \n","          # Tell pytorch not to bother with constructing the compute graph during\n","          # the forward pass, since this is only needed for backprop (training).\n","          with torch.no_grad():        \n","\n","              # Forward pass, calculate logit predictions.\n","              # token_type_ids is the same as the \"segment ids\", which \n","              # differentiates sentence 1 and 2 in 2-sentence tasks.\n","              # The documentation for this `model` function is here: \n","              # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","              # Get the \"logits\" output by the model. The \"logits\" are the output\n","              # values prior to applying an activation function like the softmax.\n","              (loss, logits) = model(b_input_ids, \n","                                    token_type_ids=b_token_type_ids, \n","  #                                    token_type_ids=None, \n","                                    attention_mask=b_input_mask,\n","                                    labels=b_labels)\n","              \n","          # Accumulate the validation loss.\n","          total_eval_loss += loss.item()\n","\n","          # Move logits and labels to CPU\n","          logits = logits.detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","\n","          # Calculate the accuracy for this batch of test sentences, and\n","          # accumulate it over all batches.\n","          total_eval_accuracy += flat_accuracy(logits, label_ids)\n","          \n","\n","      # Report the final accuracy for this validation run.\n","      avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","      print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","      # Calculate the average loss over all of the batches.\n","      avg_val_loss = total_eval_loss / len(validation_dataloader)\n","      \n","      # Measure how long the validation run took.\n","      validation_time = format_time(time.time() - t0)\n","\n","      print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","      print(\"  Validation took: {:}\".format(validation_time))\n","      # Record all statistics from this epoch.\n","      training_stats.append(\n","          {\n","              'epoch': epoch_i + 1,\n","              'Training Loss': avg_train_loss,\n","              'Valid. Loss': avg_val_loss,\n","              'Valid. Accur.': avg_val_accuracy,\n","              'Training Time': training_time,\n","              'Validation Time': validation_time\n","          }\n","      )\n","\n","      print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","  return training_stats"]},{"cell_type":"code","execution_count":24,"id":"8a81f2f2","metadata":{"ExecuteTime":{"end_time":"2022-07-28T11:53:33.885202Z","start_time":"2022-07-28T11:53:33.885202Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1659513994930,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"8a81f2f2"},"outputs":[],"source":["def training_stats(training_stats):\n","  # Display floats with two decimal places.\n","  pd.set_option('precision', 2)\n","\n","  # Create a DataFrame from our training statistics.\n","  df_stats = pd.DataFrame(data=training_stats)\n","\n","  # Use the 'epoch' as the row index.\n","  df_stats = df_stats.set_index('epoch')\n","\n","  # A hack to force the column headers to wrap.\n","  #df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","  # Display the table.\n","  df_stats"]},{"cell_type":"code","execution_count":25,"id":"efa35692","metadata":{"ExecuteTime":{"end_time":"2022-07-28T11:45:31.802417Z","start_time":"2022-07-28T11:45:31.723269Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1659513994930,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"efa35692","scrolled":true},"outputs":[],"source":["def prediction():\n","  # Prediction on test set\n","\n","  # print('Predicting labels for {:,} test sentences...'.format(len(test_dataset)))\n","\n","  # Put model in evaluation mode\n","  model.eval()\n","\n","  # Tracking variables \n","  predictions , true_labels = [], []\n","\n","  # Predict \n","  for batch in test_dataloader:\n","      # Add batch to GPU\n","  #     batch = tuple(t.to(device) for t in batch)\n","      # print(batch)\n","\n","      # Unpack the inputs from our dataloader\n","      b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n","      # print(b_input_ids)\n","\n","      # Telling the model not to compute or store gradients, saving memory and \n","      # speeding up prediction\n","      with torch.no_grad():\n","          # Forward pass, calculate logit predictions\n","          outputs = model(input_ids = b_input_ids, \n","                          token_type_ids = b_token_type_ids, \n","                          attention_mask = b_input_mask,\n","                          # labels = b_labels,\n","                          )\n","\n","      logits = outputs[0]\n","\n","      # Move logits and labels to CPU\n","      logits = logits.detach().cpu().numpy()\n","      label_ids = b_labels.to('cpu').numpy()\n","\n","      # Store predictions and true labels\n","      predictions.append(logits)\n","      true_labels.append(label_ids)\n","\n","  # print('    DONE.')"]},{"cell_type":"code","execution_count":26,"id":"A8nX8GhemrBi","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1659513994930,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"A8nX8GhemrBi"},"outputs":[],"source":["def test_prediction(test_dataloader):\n","  # Prediction on test set\n","  # Put model in evaluation mode\n","  model.eval()\n","\n","  # Tracking variables \n","  predictions , true_labels = [], []\n","\n","  # Predict \n","  for batch in test_dataloader:\n","      # Add batch to GPU\n","      # batch = tuple(t.to(device) for t in batch)\n","      # print(batch)\n","\n","      # Unpack the inputs from our dataloader\n","      b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n","      # print(b_input_ids)\n","\n","      # Telling the model not to compute or store gradients, saving memory and \n","      # speeding up prediction\n","      with torch.no_grad():\n","          # Forward pass, calculate logit predictions\n","          outputs = model(input_ids = b_input_ids, \n","                          token_type_ids = b_token_type_ids, \n","                          attention_mask = b_input_mask,\n","                          # labels = b_labels,\n","                          )\n","\n","      logits = outputs[0]\n","\n","      # Move logits and labels to CPU\n","      logits = logits.detach().cpu().numpy()\n","      label_ids = b_labels.cpu().numpy()\n","\n","      # Store predictions and true labels\n","      for logit in logits:\n","          pred = np.argmax(logit)\n","          predictions.append(pred)\n","\n","      for ids in label_ids:\n","          true_labels.append(ids)\n","  return true_labels, predictions"]},{"cell_type":"code","execution_count":27,"id":"8XWVp09Pattc","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1659513994931,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"8XWVp09Pattc"},"outputs":[],"source":["def result(true_labels, predictions):\n","  target_names = ['0', '1']\n","  print(classification_report(true_labels, predictions, target_names=target_names))\n","\n","  # Build confusion matrix\n","  cf_matrix = confusion_matrix(true_labels, predictions)\n","  print(cf_matrix)\n","\n","  df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in target_names],\n","                     columns = [i for i in target_names])\n","  \n","  plt.figure(figsize = (12,7))\n","  sns.heatmap(df_cm, annot=True)"]},{"cell_type":"markdown","id":"prZ_8xhSMv5g","metadata":{"id":"prZ_8xhSMv5g"},"source":["# here\n"]},{"cell_type":"code","execution_count":28,"id":"8e652482","metadata":{"ExecuteTime":{"end_time":"2022-07-29T01:01:46.425923Z","start_time":"2022-07-29T01:01:45.963721Z"},"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3366299,"status":"ok","timestamp":1659517361226,"user":{"displayName":"Junghoon Lee","userId":"02038086070504845152"},"user_tz":-540},"id":"8e652482","outputId":"5edf4905-05e5-410e-c7b4-eeec3c824d8f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at snunlp/KR-BERT-char16424 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-BERT-char16424 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["model : train_mecab.csv\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/19436 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|██████████| 19436/19436 [00:07<00:00, 2659.86it/s]\n","100%|██████████| 8330/8330 [00:03<00:00, 2257.64it/s]\n","100%|██████████| 6953/6953 [00:02<00:00, 2445.67it/s]\n"]},{"name":"stdout","output_type":"stream","text":["traning start\n","\n","  Average training loss: 0.47\n","  Training epcoh took: 0:03:54\n","  Accuracy: 0.79\n","  Validation Loss: 0.43\n","  Validation took: 0:00:30\n","Total training took 0:04:24 (h:mm:ss)\n","\n","  Average training loss: 0.36\n","  Training epcoh took: 0:03:53\n","  Accuracy: 0.80\n","  Validation Loss: 0.45\n","  Validation took: 0:00:30\n","Total training took 0:08:47 (h:mm:ss)\n","\n","  Average training loss: 0.25\n","  Training epcoh took: 0:03:53\n","  Accuracy: 0.79\n","  Validation Loss: 0.54\n","  Validation took: 0:00:30\n","Total training took 0:13:10 (h:mm:ss)\n","\n","  Average training loss: 0.19\n","  Training epcoh took: 0:03:53\n","  Accuracy: 0.79\n","  Validation Loss: 0.59\n","  Validation took: 0:00:30\n","Total training took 0:17:33 (h:mm:ss)\n","학습 데이터 예측 시작\n","test 예측 시작\n","test 예측 결과\n","              precision    recall  f1-score   support\n","\n","           0       0.67      0.70      0.69      2297\n","           1       0.85      0.83      0.84      4656\n","\n","    accuracy                           0.79      6953\n","   macro avg       0.76      0.77      0.76      6953\n","weighted avg       0.79      0.79      0.79      6953\n","\n","[[1616  681]\n"," [ 800 3856]]\n","--------------------------------------------------\n","--------------------------------------------------\n","model : train_khaiii.csv\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/19573 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|██████████| 19573/19573 [00:07<00:00, 2793.74it/s]\n","100%|██████████| 8389/8389 [00:02<00:00, 2953.05it/s]\n","100%|██████████| 6992/6992 [00:02<00:00, 2979.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["traning start\n","\n","  Average training loss: 0.37\n","  Training epcoh took: 0:03:55\n","  Accuracy: 0.85\n","  Validation Loss: 0.33\n","  Validation took: 0:00:30\n","Total training took 0:04:25 (h:mm:ss)\n","\n","  Average training loss: 0.24\n","  Training epcoh took: 0:03:55\n","  Accuracy: 0.85\n","  Validation Loss: 0.36\n","  Validation took: 0:00:30\n","Total training took 0:08:50 (h:mm:ss)\n","\n","  Average training loss: 0.14\n","  Training epcoh took: 0:03:55\n","  Accuracy: 0.85\n","  Validation Loss: 0.45\n","  Validation took: 0:00:30\n","Total training took 0:13:15 (h:mm:ss)\n","\n","  Average training loss: 0.09\n","  Training epcoh took: 0:03:55\n","  Accuracy: 0.85\n","  Validation Loss: 0.55\n","  Validation took: 0:00:30\n","Total training took 0:17:41 (h:mm:ss)\n","학습 데이터 예측 시작\n","test 예측 시작\n","test 예측 결과\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.71      0.71      2321\n","           1       0.86      0.85      0.85      4671\n","\n","    accuracy                           0.80      6992\n","   macro avg       0.78      0.78      0.78      6992\n","weighted avg       0.81      0.80      0.80      6992\n","\n","[[1648  673]\n"," [ 693 3978]]\n","--------------------------------------------------\n","--------------------------------------------------\n","model : train_kiwi.csv\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/19529 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|██████████| 19529/19529 [00:06<00:00, 3068.74it/s]\n","100%|██████████| 8370/8370 [00:02<00:00, 3089.68it/s]\n","100%|██████████| 6976/6976 [00:02<00:00, 3072.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["traning start\n","\n","  Average training loss: 0.28\n","  Training epcoh took: 0:03:55\n","  Accuracy: 0.91\n","  Validation Loss: 0.24\n","  Validation took: 0:00:30\n","Total training took 0:04:25 (h:mm:ss)\n","\n","  Average training loss: 0.16\n","  Training epcoh took: 0:03:54\n","  Accuracy: 0.91\n","  Validation Loss: 0.26\n","  Validation took: 0:00:30\n","Total training took 0:08:50 (h:mm:ss)\n","\n","  Average training loss: 0.09\n","  Training epcoh took: 0:03:54\n","  Accuracy: 0.90\n","  Validation Loss: 0.37\n","  Validation took: 0:00:30\n","Total training took 0:13:14 (h:mm:ss)\n","\n","  Average training loss: 0.06\n","  Training epcoh took: 0:03:54\n","  Accuracy: 0.90\n","  Validation Loss: 0.42\n","  Validation took: 0:00:30\n","Total training took 0:17:38 (h:mm:ss)\n","학습 데이터 예측 시작\n","test 예측 시작\n","test 예측 결과\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.70      0.71      2311\n","           1       0.85      0.86      0.86      4665\n","\n","    accuracy                           0.81      6976\n","   macro avg       0.78      0.78      0.78      6976\n","weighted avg       0.81      0.81      0.81      6976\n","\n","[[1624  687]\n"," [ 661 4004]]\n","--------------------------------------------------\n","--------------------------------------------------\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 864x504 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 864x504 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAngAAAGbCAYAAABXr+2LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAW1klEQVR4nO3de7CeVX0v8O/aIYFCuUMRAhgsTMHaHlGgXARjrSBYEGwPRduq6Jgzc0ZB0BFRj5wjMIp2mOP9TFoRROWiFi8BC6gEAhIhqAUkeOGiZCdAW8ALgsLe6/xBmiGSnYTVN+/77IfPh3nHvfe7knftfzJff99nPU+ptQYAgP4YG/UGAAAYLAEPAKBnBDwAgJ4R8AAAekbAAwDomY029AecNOc4x3SB9fKx5YtGvQVgmnjst+Nl5Hv49zsHlnFmbvecgf4+JngAAD2zwSd4AAC9NDkx6h1MyQQPAKBnTPAAAFrUyVHvYEoCHgBAi8nuBjwVLQBAz5jgAQA0qCpaAICeUdECADAsJngAAC1UtAAAPeNGxwAADIsJHgBACxUtAEDPOEULAMCwmOABADRwo2MAgL5R0QIAMCwmeAAALVS0AAA940bHAAAMiwkeAEALFS0AQM84RQsAwLCY4AEAtFDRAgD0jIoWAIBhMcEDAGhQa3fvgyfgAQC06PA1eCpaAICeMcEDAGjR4UMWAh4AQIsOV7QCHgBAi8nuHrJwDR4AQM+Y4AEAtFDRAgD0TIcPWahoAQB6xgQPAKCFihYAoGdUtAAADIsJHgBAiw5P8AQ8AIAGtbrRMQAAQ2KCBwDQQkULANAzHb5NiooWAKBnTPAAAFqoaAEAekZFCwDAsJjgAQC0UNECAPSMihYAgGExwQMAaKGiBQDomQ4HPBUtAEDPmOABALQY8iGLUsrdSX6ZZCLJ47XWfaZaK+ABALQYTUX7klrrv69rkYoWAKBnBDwAgBZ1cmCvUsq8UsqSJ73mrekTk1xRSrlpivdXUdECALQYYEVba52fZP46lr2o1jpeSvmDJFeWUm6vtV6zpoUmeAAA00CtdXzl/96f5JIk+021VsADAGgxwIp2XUopm5VSNv/Pr5McmuTWqdaraAEAWgz3FO0OSS4ppSRP5LfP11r/ZarFAh4AQMfVWu9M8t/Wd72ABwDQosOPKhPwAABa1DrqHUzJIQsAgJ4xwQMAaKGiBQDomQ4HPBUtAEDPmOABALRYjxsUj4qABwDQQkULAMCwmOABALTo8H3wBDwAgBYqWgAAhsUEDwCgRYcneAIeAECLDt8mRUULANAzJngAAA3qpFO0AAD90uFr8FS0AAA9Y4IHANCiw4csBDwAgBYdvgZPRQsA0DMmeAAALTp8yELAAwBoIeABAPRMdQ0eAABDYoIHANBCRcsz0VY7bpvXnP0/s/l2Wya15voLvpVrPv311dY872UvzOEnH5taayYfn8gl7/tM7lrywxHtGBi2Qw+dm7PPfl9mjI3lnE9fkA996OOrvb/rrrPzj/PPzvbbb5MHHngor3v9CRkfX5Ekef/7353DD39pxsbG8s1vXJOTTn7vKH4Fnsk6fJsUAY8NZvLxiXz1jPOz7Ad3Z+PNNsnJX3t/frjo5tz3k/FVa3503a259cqbkiQ77rlrXvfxE/OBl75tVFsGhmhsbCwf+fCZOfyIV2fZshVZfP1lWbDgiixd+uNVa84667357Oe+mPPP/0Lmzj0oZ55xal5//Ak5YP99cuAB++YFL/iLJMnVC7+cQw45INdcc/2ofh3oFNfgscH84t8eyrIf3J0k+c3Dj+a+O8az5bO2WW3Nb3/9m1Vfz9p046S7/2cIGLD99t07d9xxd+6662d57LHHctHFX8mRRx622pq99tojV111XZJk4cLrcuSRhyZJaq3ZZJONM2vWrGy88azMnLlR7r//34b+O/AMVycH9xqwdU7wSil7JnllktkrfzSe5Ku11qUD3w29tfXO22fn587JT7//k6e89yeH7ZtXvOO4/P62W+Yf33DWCHYHjMJOs5+VZcuWr/p+fHxF9tt379XW3HzzbTnm6MPz0Y99KkcffXi22GLzbLPN1ln8nZuycOG3c8/PvptSSj7xyXNz++1P/fcFNqgOV7RrneCVUk5JcmGSkuSGla+S5IJSyjvX8ufmlVKWlFKW3PLLOwa5X6ahWZtunOM/eVIued95+c2vHnnK+7dcfmM+8NK35Zx5/5AjTj52BDsEuuqUU07PwYfsnxtvuDyHHLx/li1bkYmJifzhH87JnnvukTm77ZNnz3lhXjL3oBx00H6j3i50xromeG9M8se11see/MNSytlJfpDkA2v6Q7XW+UnmJ8lJc47rbrxlgxvbaEaO/38n56YvX5tbLr9xrWvvvOH2bLvrH2SzrTfPww/+ckg7BEZl+fi92XnnnVZ9P3v2jhlffu9qa1asuC/HHvumJMlmm22aY455RX7+81/kjW98Tb5zw3fz8MO/TpL8y+Xfyv77vzDXXXfD8H4BnvFqh0/RrusavMkkO63h5zuufA/W6riz/kfu+8l4rv7UZWt8f7tn77Dq653/eE5mzJop3MEzxI1Lvp/dd98tc+bskpkzZ+Zvjn1lFiy4YrU12267dUopSZJTTnlLzj3vwiTJPfcszyEH758ZM2Zko402yiEHH6CiZfgm6+BeA7auCd5bk3yzlPLjJPes/NmuSXZP8uaB74Ze2W2fP8q+f3VIli/9ad5+2RPD3ks/eGG2nr1dkuTbn/tG/vTwP8u+rzo4E49P5LFHf5vPvPnDo9wyMEQTExM58a3vyaWXfj4zxsZy7nkX5bbbfpTTTnt7brrpX7NgwZV58YsPzBmnn5qammsXLc5bTnh3kuRLX1qQl8w9KN/73jdTa80Vly/MpZdeOeLfCLqj1HU8ZqOUMpZkv6x+yOLGWuvE+nyAihZYXx9bvmjUWwCmicd+O15GvYeHz/i7gWWczd7z2YH+Pus8RVtrnUyyeJAfCgAw7U3XU7QAAEw/nmQBANCiw6doBTwAgBYqWgAAhsUEDwCgxQZ4huygCHgAAC1UtAAADIsJHgBAgy4/i1bAAwBooaIFAGBYTPAAAFp0eIIn4AEAtOjwbVJUtAAAPWOCBwDQQkULANAvtcMBT0ULANAzJngAAC06PMET8AAAWnT4SRYqWgCAnjHBAwBooaIFAOiZDgc8FS0AQM+Y4AEANKi1uxM8AQ8AoIWKFgCAYTHBAwBo0eEJnoAHANDAs2gBABgaEzwAgBYdnuAJeAAALbr7KFoVLQBA35jgAQA06PIhCwEPAKBFhwOeihYAoGdM8AAAWozgkEUpZUaSJUnGa61/OdU6AQ8AoMGIrsE7McnSJFusbZGKFgBgGiil7JzkFUn+aV1rBTwAgBaTg3uVUuaVUpY86TVvDZ/4f5O8I+tRDqtoAQAaDLKirbXOTzJ/qvdLKX+Z5P5a602llLnr+vtM8AAAuu+gJEeVUu5OcmGSPy+lfHaqxQIeAECLAVa061JrPbXWunOtdU6S45J8q9b6d1OtV9ECADSoHX4WrYAHANBiRAGv1rowycK1rVHRAgD0jAkeAEADFS0AQN90OOCpaAEAesYEDwCggYoWAKBnuhzwVLQAAD1jggcA0KDLEzwBDwCgRS2j3sGUVLQAAD1jggcA0EBFCwDQM3VSRQsAwJCY4AEANFDRAgD0THWKFgCAYTHBAwBooKIFAOgZp2gBABgaEzwAgAa1jnoHUxPwAAAaqGgBABgaEzwAgAZdnuAJeAAADbp8DZ6KFgCgZ0zwAAAaqGgBAHrGs2gBABgaEzwAgAaeRQsA0DOTKloAAIbFBA8AoEGXD1kIeAAADbp8mxQVLQBAz5jgAQA06PKjygQ8AIAGKloAAIbGBA8AoEGX74Mn4AEANOjybVJUtAAAPWOCBwDQwClaAICe6fI1eCpaAICeMcEDAGjQ5UMWAh4AQIMuX4OnogUA6JkNPsH76PJFG/ojgJ54xL8XwDTS5UMWKloAgAZdvgZPRQsA0DMmeAAADVS0AAA90+FDtAIeAECLLk/wXIMHANAzJngAAA26fIpWwAMAaDA56g2shYoWAKBnTPAAABrUqGgBAHplssP3SVHRAgD0jAkeAECDSRUtAEC/dPkaPBUtAEDPmOABADTo8n3wBDwAgAYqWgAAhsYEDwCggYoWAKBnuhzwVLQAAD1jggcA0KDLhywEPACABpPdzXcqWgCAvjHBAwBoMMxn0ZZSNklyTZKN80R++2Kt9bSp1gt4AAAN6nA/7jdJ/rzW+qtSyswk15ZSvl5rXbymxQIeAEDH1Vprkl+t/HbmyteUGdM1eAAADSYH+CqlzCulLHnSa97vfl4pZUYp5ftJ7k9yZa31O1PtzQQPAKDBZBncNXi11vlJ5q9jzUSS55dStkpySSnlebXWW9e01gQPAGAaqbU+lOSqJC+fao2ABwDQoA7wtS6llO1XTu5SSvm9JC9LcvtU61W0AAANhvws2h2TnFdKmZEnBnQX11oXTLVYwAMA6Lha681J9l7f9QIeAECDLj+qTMADAGgwzCdZPF0OWQAA9IwJHgBAgyE/quxpEfAAABp0+Ro8FS0AQM+Y4AEANBjyffCeFgEPAKBBl6/BU9ECAPSMCR4AQIMuH7IQ8AAAGnT5GjwVLQBAz5jgAQA06PIET8ADAGhQO3wNnooWAKBnTPAAABqoaAEAeqbLAU9FCwDQMyZ4AAANuvyoMgEPAKBBl59koaIFAOgZEzwAgAZdPmQh4AEANOhywFPRAgD0jAkeAEADp2gBAHqmy6doBTwAgAauwQMAYGhM8AAAGrgGDwCgZyY7HPFUtAAAPWOCBwDQoMuHLAQ8AIAG3S1oVbQAAL1jggcA0EBFCwDQM11+koWKFgCgZ0zwAAAadPk+eAIeAECD7sY7FS0AQO+Y4AEANHCKFgCgZ7p8DZ6KFgCgZ0zwAAAadHd+J+ABADTp8jV4KloAgJ4xwQMAaNDlQxYCHgBAg+7GOxUtAEDvmOABADTo8iELAQ8AoEHtcEmrogUA6BkTPACABipaAICe6fJtUlS0AAA9Y4IHANCgu/M7AQ8AoImKFgCAoTHBY+AOO3Ruzj77fZkxNpZzPn1BPvihj6/2/q67zs4/zT87222/TR584KG89vUnZHx8RZLkN4/8LLfcenuS5J57xnPMq44f+v6B0Tj0r16XzTbdNGNjY5kxY0YuPucjq71/zue+mEuvuCpJMjExkTt/ek8WXXphttxi81FsF5yi5ZljbGwsH/nwmXn5Ea/OsmUrsvj6y/K1BVdk6dIfr1rzwbPem/M/98Wcf/4X8pK5B+XMM07N648/IUnyyCOPZp99Dx3V9oERO+ejH8jWW225xvfe8Ld/nTf87V8nSRZeuzifuejLwh0j5UbHPGPst+/eueOOu3PXXT/LY489losv/kqOOvKw1dbstdceueqq65IkVy28LkcdKdABT89l37g6R7zsxaPeBnSWgMdA7TT7Wbln2fJV3y8bX5GddnrWamtuvvm2HHP04UmSo48+PFtssXm22WbrJMkmm2ycxddflusWfS1HHbV6MAT6rZSSeSe9O8e+4S35wlcum3LdI48+mmsXL8nL5r5oiLuDp5oc4GvQmivaUsrxtdZPT/HevCTzkqTM2DJjY5u1fgw99I5TTs9HPnxGXvvaY7No0eIsW7YiExMTSZLn7P5nWb783uy226658vKLc+utt+fOO3864h0Dw/CZT/5Ddth+u/zHgw/lTW99V3Z79i7Z5/l/8pR1C6/9Tvb+0+eqZxm5vla0/2eqN2qt82ut+9Ra9xHunlmWj9+bXXbeadX3O8/eMcuX37vamhUr7st/P/ZN2Xe/w/K/3ntWkuTnP//FE39+5dq77vpZrr7m+jz/+c8b0s6BUdth++2SJNtuvVVeesiBueW2H65x3de/eXWO+Iu5Q9wZTD9rDXillJuneN2SZIch7ZFp5MYl38/uu++WOXN2ycyZM3Pssa/M1xZcsdqabbfdOqWUJMk7T3lLzj3vwiTJVlttmVmzZq1ac+AB+2bp0h8N9xcARuLXjzyahx/+9aqvv33Dd7PHc+Y8Zd0vf/Vwlnzvlrzk4AOGvEN4qulc0e6Q5LAkD/7Oz0uSb2+A/TDNTUxM5MS3vieXXfr5zBgby7nnXZTbbvtR/vdpb8+Sm/41CxZcmRe/+MCcefqpqalZtGhx3nLCu5Mke+25Rz7xiQ9kcrJmbKzkgx/62Gqnb4H++o8HHsyJ7zo9STLx+ESOOHRuXrT/PrnokkuTJH9zzCuSJN+8+ts5cL8XZNPf22Rke4X/NFm7W9GWupbNlVI+leTTtdZr1/De52utr1nXB2w0a3Z3f3ugUx5ZvmjUWwCmiZnbPaeMeg9//+xXDSzjnP/Tfx7o77PWCV6t9Y1reW+d4Q4AoK+6PMFyo2MAgAaeRQsAwNAIeAAADeoA/1uXUsoupZSrSim3lVJ+UEo5cW3rVbQAAA02xO1N1uLxJG+rtX63lLJ5kptKKVfWWm9b02ITPACAjqu1rqi1fnfl179MsjTJ7KnWm+ABADQY5CGLJz/mdaX5tdb5U6ydk2TvJN+Z6u8T8AAAGgzyWbQrw9waA92TlVJ+P8mXkry11vqLqdapaAEApoFSysw8Ee4+V2v957WtNcEDAGgwzEMW5YmHuH8qydJa69nrWi/gAQA0WNvjXjeAg5L8fZJbSinfX/mzd9VaL1vTYgEPAKDjaq3XJlnv59UKeAAADbr8qDIBDwCgwZBvdPy0CHgAAA0GeZuUQXObFACAnjHBAwBo4Bo8AICeGfJtUp4WFS0AQM+Y4AEANHCKFgCgZ5yiBQBgaEzwAAAaOEULANAzTtECADA0JngAAA1UtAAAPeMULQAAQ2OCBwDQYLLDhywEPACABt2NdypaAIDeMcEDAGjgFC0AQM90OeCpaAEAesYEDwCgQZcfVSbgAQA0UNECADA0JngAAA26/KgyAQ8AoEGXr8FT0QIA9IwJHgBAgy4fshDwAAAaqGgBABgaEzwAgAYqWgCAnunybVJUtAAAPWOCBwDQYLLDhywEPACABipaAACGxgQPAKCBihYAoGdUtAAADI0JHgBAAxUtAEDPqGgBABgaEzwAgAYqWgCAnlHRAgAwNCZ4AAANap0c9RamJOABADSYVNECADAsJngAAA2qU7QAAP2iogUAYGhM8AAAGqhoAQB6pstPslDRAgD0jAkeAECDLj+qTMADAGjgGjwAgJ5xmxQAAIbGBA8AoIGKFgCgZ9wmBQCAoTHBAwBooKIFAOgZp2gBABgaEzwAgAYqWgCAnnGKFgCAoTHBAwBoUDt8yELAAwBooKIFAGBoBDwAgAa11oG91kcp5ZxSyv2llFvXtVbAAwBoUAf433o6N8nL12ehgAcAMA3UWq9J8sD6rHXIAgCgwSBvdFxKmZdk3pN+NL/WOr/17xPwAAAaDDLgrQxzzYHud6loAQB6xgQPAKBBd++Cl5QuPyiX/iqlzPuvXFsAPHP49wKeUEq5IMncJNsluS/JabXWT61xrYDHKJRSltRa9xn1PoDu8+8FPH2uwQMA6BkBDwCgZwQ8RsX1NMD68u8FPE2uwQMA6BkTPACAnhHwAAB6RsBj6EopLy+l/LCU8pNSyjtHvR+gm0op55RS7i+l3DrqvcB0I+AxVKWUGUk+nuTwJM9N8upSynNHuyugo85N8vJRbwKmIwGPYdsvyU9qrXfWWn+b5MIkrxzxnoAOqrVek+SBUe8DpiMBj2GbneSeJ32/bOXPAIABEfAAAHpGwGPYxpPs8qTvd175MwBgQAQ8hu3GJHuUUnYrpcxKclySr454TwDQKwIeQ1VrfTzJm5NcnmRpkotrrT8Y7a6ALiqlXJDk+iR/VEpZVkp546j3BNOFR5UBAPSMCR4AQM8IeAAAPSPgAQD0jIAHANAzAh4AQM8IeAAAPSPgAQD0zP8HHSlHvFJbESAAAAAASUVORK5CYII=","text/plain":["<Figure size 864x504 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["train_path = \"/content/drive/MyDrive/NLP미니플젝-10조/태훈/코드/형태소분석기/data3/train/\"\n","test_path = \"/content/drive/MyDrive/NLP미니플젝-10조/태훈/코드/형태소분석기/data3/test/\"\n","\n","train_okt = \"train_okt\" + \".csv\"\n","train_kkma = \"train_kkma\" + \".csv\"\n","train_komoran = \"train_komoran\" + \".csv\"\n","train_hannanum = \"train_hannanum\" + \".csv\"\n","train_mecab = \"train_mecab\" + \".csv\"\n","train_khaiii = \"train_khaiii\" + \".csv\"\n","train_kiwi = \"train_kiwi\" + \".csv\"\n","\n","test_okt = \"test_okt\" + \".csv\"\n","test_kkma = \"test_kkma\" + \".csv\"\n","test_komoran = \"test_komoran\" + \".csv\"\n","test_hannanum = \"test_hannanum\" + \".csv\"\n","test_mecab = \"test_mecab\" + \".csv\"\n","test_khaiii = \"test_khaiii\" + \".csv\"\n","test_kiwi = \"test_kiwi\" + \".csv\"\n","\n","trains = [train_mecab, train_khaiii,train_kiwi]\n","tests = [test_mecab, test_khaiii,test_kiwi]\n","\n","epo = 4 # 에폭\n","batch_size = 32 # 배치사이즈\n","MAX_LENGTH = 128\n","\n","\n","#model_name = \"bert-base-uncased\" # bert\n","model_name = \"snunlp/KR-BERT-char16424\" # kr-bert\n","\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    model_name, # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n","    return_dict = False,\n",")\n","\n","model.to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","epochs = epo\n","\n","for i,j in zip(trains, tests):\n","  import torch, gc\n","  gc.collect()\n","  torch.cuda.empty_cache()\n","  \n","  print('model :' , i)\n","  train_data_path = train_path + i\n","  test_data_path = test_path + j\n","  df_train = pd.read_csv(train_data_path)\n","  df_test = pd.read_csv(test_data_path)\n","\n","  df_train, df_valid = train_test_split(df_train, test_size=0.3, random_state=42)\n","\n","  train_dataset = make_input(df_train)\n","  valid_dataset = make_input(df_valid)\n","  test_dataset = make_input(df_test)\n","\n","\n","  train_dataloader = DataLoader(\n","              train_dataset,  # The training samples.\n","              sampler = RandomSampler(train_dataset), # Select batches randomly\n","              batch_size = batch_size # Trains with this batch size.\n","          )\n","  validation_dataloader = DataLoader(\n","              valid_dataset, # The validation samples.\n","              sampler = SequentialSampler(valid_dataset), # Pull out batches sequentially.\n","              batch_size = batch_size # Evaluate with this batch size.\n","          )\n","  test_dataloader = DataLoader(\n","              test_dataset, # The validation samples.\n","              sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n","              batch_size = batch_size # Evaluate with this batch size.\n","          )\n","  total_steps = len(train_dataloader) * epochs\n","  scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                              num_warmup_steps = 0, # Default value in run_glue.py\n","                                              num_training_steps = total_steps)\n","  \n","  print('traning start')\n","  training_statss = training(train_dataloader, validation_dataloader)\n","  training_stats(training_statss)\n","  \n","  print('학습 데이터 예측 시작')\n","  prediction()\n","  print('test 예측 시작')\n","  true_labels, predictions = test_prediction(test_dataloader)\n","  print('test 예측 결과')\n","  result(true_labels, predictions)\n","  print('-'*50)\n","  print('-'*50)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"BERT_Pytorch2_lastest222.ipynb","provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.1 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.1"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"vscode":{"interpreter":{"hash":"e1d54daf239baa0d08258bc0b12cc3f117e61421667d5d2e43f96de704b4c513"}}},"nbformat":4,"nbformat_minor":5}
